# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nRK7IZUj99TwTOkdVf6U1-QTHEmAls8w
"""

!pip install transformers==4.33.0 torch==2.0.1 google-api-python-client matplotlib scikit-learn pandas

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import googleapiclient.discovery
import pandas as pd

# Custom Dataset class
class SentimentDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')
        # Flatten the tensors
        input_ids = encoding['input_ids'].squeeze(0)
        attention_mask = encoding['attention_mask'].squeeze(0)
        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': torch.tensor(label)}

# Preprocess comments for BERT
def preprocess_for_finetuning(comments, labels, tokenizer, max_length=128):
    dataset = SentimentDataset(comments, labels, tokenizer, max_length)
    return dataset

# Fine-tune BERT model
def fine_tune_model(tokenizer, model, comments, labels):
    train_comments, val_comments, train_labels, val_labels = train_test_split(comments, labels, test_size=0.2)
    train_dataset = preprocess_for_finetuning(train_comments, train_labels, tokenizer)
    val_dataset = preprocess_for_finetuning(val_comments, val_labels, tokenizer)

    training_args = TrainingArguments(
        output_dir="./results",
        num_train_epochs=3,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=64,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir="./logs",
        evaluation_strategy="epoch",
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
    )

    trainer.train()
    return model

# Fetch YouTube comments using the API with pagination
def fetch_youtube_comments(youtube, video_id, max_results=50):
    comments = []
    next_page_token = None

    try:
        while True:
            request = youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                maxResults=max_results,
                pageToken=next_page_token,
                textFormat="plainText"
            )
            response = request.execute()

            for item in response['items']:
                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
                comments.append(comment)

            next_page_token = response.get('nextPageToken')

            if not next_page_token or len(comments) >= 500:
                break

    except googleapiclient.errors.HttpError as e:
        print(f"An error occurred: {e}")
        if 'videoId' in str(e):
            print("The provided video ID may be incorrect or the video may not exist.")
        return []

    return comments

# Preprocess comments for BERT evaluation
def preprocess_comments_batch(comments_batch, tokenizer, max_length=128):
    encoding = tokenizer(comments_batch, padding="max_length", truncation=True, max_length=max_length, return_tensors="pt", is_split_into_words=False)
    return encoding

# Evaluate comments using the fine-tuned BERT model
def evaluate_comments(model, encoded_comments):
    with torch.no_grad():
        outputs = model(**encoded_comments)
        predictions = torch.argmax(outputs.logits, dim=1)
    return predictions

# Analyze and visualize sentiment distribution
def analyze_and_visualize_sentiments(categorized_comments):
    labels = ['negative', 'neutral', 'positive']
    sentiment_counts = [0, 0, 0]

    for label in labels:
        sentiment_counts[labels.index(label)] = len(categorized_comments[label])

    for label in labels:
        print(f"\n{label.capitalize()} Comments:")
        for comment in categorized_comments[label]:
            print(f"- {comment}")

    plt.figure(figsize=(8, 5))
    plt.bar(labels, sentiment_counts, color=['red', 'gray', 'green'])
    plt.xlabel('Sentiment')
    plt.ylabel('Number of Comments')
    plt.title('Distribution of Sentiment in YouTube Comments')
    plt.show()

# Main function
def main():
    api_key = 'AIzaSyB4IrwRm9Yo7y2xwGa0UjhrlSLTcxU0g18'  # Replace with your YouTube API key
    youtube = googleapiclient.discovery.build("youtube", "v3", developerKey=api_key)
    video_id = 'ZqvZpHnti9s'  # Replace with your video ID
    comments = fetch_youtube_comments(youtube, video_id, max_results=50)

    if not comments:
        print("No comments found or unable to fetch comments.")
        return

    # For demonstration, ensure we have enough labels
    num_comments = len(comments)
    if num_comments < 60:
        print("Not enough comments for training. Ensure you have at least 60 comments.")
        return

    labels = [0] * 20 + [1] * 20 + [2] * 20  # This should match the number of comments you are using

    if len(labels) != len(comments[:60]):
        print(f"Number of labels ({len(labels)}) does not match number of comments ({len(comments[:60])})")
        return

    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=3)

    model = fine_tune_model(tokenizer, model, comments[:60], labels)  # Ensure that comments and labels match

    model.eval()
    batch_size = 16
    categorized_comments = {'negative': [], 'neutral': [], 'positive': []}

    for i in range(0, len(comments), batch_size):
        batch = comments[i:i+batch_size]
        encoded_comments = preprocess_comments_batch(batch, tokenizer)
        predictions = evaluate_comments(model, encoded_comments)

        for comment, pred in zip(batch, predictions):
            label = ['negative', 'neutral', 'positive'][pred]
            categorized_comments[label].append(comment)

    analyze_and_visualize_sentiments(categorized_comments)

if __name__ == "__main__":
    main()